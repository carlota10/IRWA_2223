{"cells":[{"cell_type":"markdown","source":"# PROJECT IRWA 2022","metadata":{"tags":[],"cell_id":"db3a0a7650034e3295de1a602407b5af","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"## Part 1: Text Processing","metadata":{"tags":[],"cell_id":"1dacdd326c5f4cc78fa5e72a3dca1483","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"1. Take into account that for future queries, the final output must return (when\r\npresent) the following information for each of the selected documents: Tweet |\r\nUsername | Date | Hashtags | Likes | Retweets | Url (here the “Url” means the\r\ntweet link).","metadata":{"tags":[],"style":"decimal","number":1,"cell_id":"150cf262e27940d4857199ec725291ad","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":210,"fromCodePoint":150}],"deepnote_cell_type":"text-cell-number"}},{"cell_type":"markdown","source":"2. Think about how to handle the hashtags from your pre-processing steps (e.g.,\r\nremoving the “#” from the word), since it may be useful to involve them as separate terms\r\ninside the inverted index.","metadata":{"tags":[],"style":"decimal","number":2,"cell_id":"360a8122-2f7a-4b49-b590-b3aef564cd63","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-number"}},{"cell_type":"markdown","source":"The suggested library that may help you in stemming and stopwords: nltk","metadata":{"tags":[],"style":"decimal","number":3,"cell_id":"0d7a2626-9628-443e-b7b6-a092c32aab62","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":71,"fromCodePoint":67}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#We do all the imports\nimport nltk\nnltk.download('stopwords') #Dowload list of stopwords\n\nfrom collections import defaultdict\nfrom array import array\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport math\nimport numpy as np\nimport collections\nfrom numpy import linalg as la\nimport re #Library used to remove certain symbols / characters from a text","metadata":{"tags":[],"cell_id":"cc85a793f8694d11ad35da7afb0e4846","source_hash":"e6d0611f","execution_start":1666345787757,"execution_millis":1956,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pwd\n!ls","metadata":{"tags":[],"cell_id":"bff8f1b34e8b4087ae5a2cf03f7782fa","source_hash":"e80c3ebf","execution_start":1666345789721,"execution_millis":2188,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"/work\ndata  deepnote_exports\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# you can use pandas to explore your files\nimport pandas as pd\nimport json\n\n# Read datasets\ntweet_document_ids_map = pd.read_csv(r'data/tweet_document_ids_map.csv', header = None, delimiter = \"\\t\")","metadata":{"tags":[],"cell_id":"a3f4566f8dbf402889aea51594163fdc","source_hash":"477d98f9","execution_start":1666345791915,"execution_millis":13,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"tweet_document_ids_map.head(5)","metadata":{"tags":[],"cell_id":"f34df0c44a964f40931c73900d6dc5fc","source_hash":"f6477d60","execution_start":1666345792005,"execution_millis":33,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":5,"columns":[{"name":0,"dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"doc_1","count":1},{"name":"doc_2","count":1},{"name":"3 others","count":3}]}},{"name":1,"dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":"1575918119251419136","max":"1575918182698979328","histogram":[{"bin_start":1575918119251419100,"bin_end":1575918125596175000,"count":1},{"bin_start":1575918125596175000,"bin_end":1575918131940931000,"count":0},{"bin_start":1575918131940931000,"bin_end":1575918138285687300,"count":1},{"bin_start":1575918138285687300,"bin_end":1575918144630443300,"count":1},{"bin_start":1575918144630443300,"bin_end":1575918150975199200,"count":0},{"bin_start":1575918150975199200,"bin_end":1575918157319955200,"count":1},{"bin_start":1575918157319955200,"bin_end":1575918163664711200,"count":0},{"bin_start":1575918163664711200,"bin_end":1575918170009467400,"count":0},{"bin_start":1575918170009467400,"bin_end":1575918176354223400,"count":0},{"bin_start":1575918176354223400,"bin_end":1575918182698979300,"count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"0":"doc_1","1":"1575918182698979328","_deepnote_index_column":"0"},{"0":"doc_2","1":"1575918151862304768","_deepnote_index_column":"1"},{"0":"doc_3","1":"1575918140839673873","_deepnote_index_column":"2"},{"0":"doc_4","1":"1575918135009738752","_deepnote_index_column":"3"},{"0":"doc_5","1":"1575918119251419136","_deepnote_index_column":"4"}]},"text/plain":"       0                    1\n0  doc_1  1575918182698979328\n1  doc_2  1575918151862304768\n2  doc_3  1575918140839673873\n3  doc_4  1575918135009738752\n4  doc_5  1575918119251419136","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>doc_1</td>\n      <td>1575918182698979328</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>doc_2</td>\n      <td>1575918151862304768</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>doc_3</td>\n      <td>1575918140839673873</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>doc_4</td>\n      <td>1575918135009738752</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>doc_5</td>\n      <td>1575918119251419136</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"doc_ID = {} #Create a dictinary to save the docID\nfor i in range(len(tweet_document_ids_map)): #Iterate through the csv file rows\n    #Store the docID in the dictionary accessing with the tweetID\n    doc_ID[tweet_document_ids_map[1].iloc[i]] = tweet_document_ids_map[0].iloc[i]","metadata":{"tags":[],"cell_id":"cd603cee521949329204d1c6379abd8c","source_hash":"2bd49f97","execution_start":1666345792134,"execution_millis":201,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#We open the json file and extract the tweets in it\nwith open(\"data/tw_hurricane_data.json\", \"r\") as json_file:\n    raw_tweets = [json.loads(tweet) for tweet in json_file]","metadata":{"tags":[],"cell_id":"22994e68cfe447dba62c08e9a669f39a","source_hash":"75c36055","execution_start":1666345792337,"execution_millis":889,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tweets = {} #We create a dictionary to save all the needed tweets' information\n\nfor tweet in raw_tweets: #Iterate through all tweets in the json file\n    dict_tweet = {} #Create a dictinary to save each individual tweet's information\n    dict_tweet['Tweet'] = tweet['full_text'] #Store the text of the tweet\n    dict_tweet['Date'] = tweet['created_at'] #Store the date of the tweet creation\n    dict_tweet['Hashtags']= [] #Create an empty list to store the hashtags in the tweet\n    for hashtag in tweet['entities']['hashtags']: #Iterate through the hashtag dictionary\n        dict_tweet['Hashtags'].append(hashtag['text']) #Append the hashtag to the list\n    dict_tweet['Username'] = tweet['user']['screen_name'] #Store the username of the \"writer\"\n    dict_tweet['Likes'] = tweet['favorite_count'] #Store the likes count of the tweet\n    dict_tweet['Retweets'] = tweet['retweet_count'] #Store the retweets count of the tweet\n\n    #We \"create\" the URL of each tweet and store it\n    #https://twitter.com/screen_name/status/tweet_id\n    dict_tweet['Url'] = 'https://twitter.com/'+dict_tweet['Username']+'/status/'+tweet['id_str']\n\n    dict_tweet['Doc_ID'] = doc_ID[tweet['id']]\n    #add tweet to dictionary tweets with the id as the tweet key\n    tweets[tweet['id']] = dict_tweet\n","metadata":{"tags":[],"cell_id":"63bf67708eef47f7ba60415b7ed64d4c","source_hash":"8ef9a402","execution_start":1666345793231,"execution_millis":33,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#Print an example of a stored tweet information entry from the dictionary\ntweets[list(tweets.keys())[2570]]","metadata":{"tags":[],"cell_id":"625f550e0cde4403bfb6360e8f48cc60","source_hash":"a331892b","execution_start":1666345793278,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"{'Tweet': '@Next_Gen_X $kerri0922 would help with #HurricaneIan clean up. https://t.co/SuDL5LStQu',\n 'Date': 'Fri Sep 30 15:46:52 +0000 2022',\n 'Hashtags': ['HurricaneIan'],\n 'Username': 'NewKerristartin',\n 'Likes': 1,\n 'Retweets': 0,\n 'Url': 'https://twitter.com/NewKerristartin/status/1575874831190413312',\n 'Doc_ID': 'doc_2571'}"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#This function that receives a line of text as input will return a list of the words\n#contained in it after having removed the stopwords and non-important symbols, \n#transforming to lowercase, tokenizing and stemming.\ndef build_terms(line):\n    \"\"\"\n    Preprocess the article text (title + body) removing stop words, stemming,\n    transforming in lowercase and return the tokens of the text.\n    \n    Argument:\n    line -- string (text) to be preprocessed\n    \n    Returns:\n    line - a list of tokens corresponding to the input text after the preprocessing\n    \"\"\"\n\n    stemmer = PorterStemmer()\n\n    stop_words = set(stopwords.words(\"english\"))\n    line = line.lower()  #Convert to lowercase\n    line = line.split()  # Tokenize the text to get a list of terms\n    line = [x for x in line if x not in stop_words]  # eliminate the stopwords\n    line = [x for x in line if x.startswith((\"@\", \"https://\")) != True]  # eliminate mentions\n    line = [re.sub('[^a-z]+', '', x) for x in line] # since it's in english we don't have to worry about accents and such\n    line = [stemmer.stem(word) for word in line] # perform stemming (HINT: use List Comprehension)\n    return line","metadata":{"tags":[],"cell_id":"cf48fbabc6f249e9b0e7f287c000f00b","source_hash":"deb501fc","execution_start":1666345793281,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#We apply the function build_terms(line) to the text section of each tweet\nfor tweet in tweets.keys():\n    tweets[tweet]['Tweet'] = build_terms(tweets[tweet]['Tweet'])","metadata":{"tags":[],"cell_id":"0b5daf3fba2e42e2abc1f513388677e7","source_hash":"382e69bf","execution_start":1666345793311,"execution_millis":1954,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#Print the same example as above to see the result of the text section of the tweet after\n#having applied the function build_terms(line)\ntweets[list(tweets.keys())[2570]]   ","metadata":{"tags":[],"cell_id":"3aa40b0f57154b8f87d4dfe0130650cb","source_hash":"ecb25a68","execution_start":1666345795267,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"{'Tweet': ['kerri', 'would', 'help', 'hurricaneian', 'clean', 'up'],\n 'Date': 'Fri Sep 30 15:46:52 +0000 2022',\n 'Hashtags': ['HurricaneIan'],\n 'Username': 'NewKerristartin',\n 'Likes': 1,\n 'Retweets': 0,\n 'Url': 'https://twitter.com/NewKerristartin/status/1575874831190413312',\n 'Doc_ID': 'doc_2571'}"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f3aafe6d-e8f4-4bba-9fe3-867404489d78' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"57c359a3cc8f477997ea359238976c1e","deepnote_execution_queue":[]}}